# ğŸ¤Ÿ SignBridge â€” ASL to Text in Your Browser

A real-time American Sign Language (ASL) to text translator that runs entirely in your browser. Uses your webcam with MediaPipe hand tracking and a lightweight gesture classifier to recognize signs and build a live transcript.

**No server required. No sign-up. No data leaves your device.**

## âœ¨ Features

- **Real-time hand tracking** â€” MediaPipe Hands draws a skeletal overlay on your hand at 30+ FPS
- **10 built-in ASL signs** â€” Hello, Thank You, Yes, No, Please, Sorry, Help, I Love You, Good, Stop
- **Train custom signs** â€” record any gesture from your webcam to teach new signs
- **Calibrate existing signs** â€” improve accuracy by recording your version of built-in signs
- **Export / Import training data** â€” share trained models between devices or collaborate with others
- **Text-to-speech** â€” speak your transcript aloud with one click
- **Copy to clipboard** â€” quickly paste your translated text anywhere
- **Two detection modes** â€” "Always Top Prediction" for responsiveness or "55% Threshold" for accuracy
- **Demo mode** â€” try the interface without a camera by clicking signs in the sidebar
- **Zero dependencies** â€” no TensorFlow, no Python, no server. One HTML file.

## ğŸš€ Quick Start

### Option 1: Live Demo (recommended)

Visit the deployed site on Netlify â€” webcam works over HTTPS with no setup:

ğŸ‘‰ https://your-site.netlify.app

### Option 2: Local server

```bash
# Clone the repo
git clone https://github.com/denishaversa/signbridge.git
cd signbridge

# Serve locally (needed for webcam access)
python3 -m http.server 8000
```

Then open [http://localhost:8000](http://localhost:8000) in Chrome or Edge.

### Option 3: Open directly

You can open `index.html` directly in your browser, but webcam access will be blocked. Use **Demo Mode** to explore the interface without a camera.

## ğŸ–ï¸ Supported Signs

| Sign | Gesture |
|------|---------|
| Hello | Open hand wave near forehead |
| Thank You | Flat hand from chin outward |
| Yes | Closed fist (nod motion) |
| No | Index + middle finger snap to thumb |
| Please | Flat hand circles on chest |
| Sorry | Fist circles on chest |
| Help | Fist on flat palm, lift up |
| I Love You | ğŸ¤Ÿ Thumb + index + pinky extended |
| Good | Thumbs up |
| Stop | Flat hand, palm facing out |

## ğŸ“ Training Custom Signs

1. Click **Train Sign** in the controls bar
2. Select **+ New Sign** tab and enter a name
3. Hold your gesture in front of the camera
4. Click **âº Start Recording** â€” hold for 3-5 seconds (aim for 30+ samples)
5. Click **â¹ Stop Recording** â†’ **Save**
6. Your sign is immediately active in the classifier

## ğŸ¯ Calibrating Existing Signs

If a built-in sign has low confidence for your hand/camera setup:

1. Click **Train Sign** â†’ select **ğŸ¯ Calibrate Existing** tab
2. Pick the sign from the dropdown
3. Record your version of the gesture (30+ samples recommended)
4. Save â€” the classifier blends your data with the built-in model

Calibrated signs show a blue dot in the sidebar. Click **â†º** in the Sign Studio to reset to defaults.

## ğŸ“¤ Sharing Training Data

Training data can be exported and shared between devices:

1. **Export** â€” click ğŸ“¤ to download a JSON file with all your custom signs and calibrations
2. **Import** â€” click ğŸ“¥ on another device to load the file
3. Continue training on top of imported data â€” contributions stack

This enables collaborative model building: multiple people can each calibrate signs and merge their data for a model that works across different hands and environments.

## âš™ï¸ Detection Modes

The **Detection Rules** card in the sidebar lets you switch between:

| Mode | Confidence | Cooldown | Best for |
|------|-----------|----------|----------|
| âš¡ Top Prediction | Any (always accepts) | 2.0s | Responsiveness, casual use |
| ğŸ¯ 55% Threshold | â‰¥ 55% required | 1.8s | Accuracy, reducing false positives |

Both modes require the sign to be held for ~10 consecutive frames (~0.5s) to confirm.

## ğŸ—ï¸ How It Works

```
Webcam â†’ MediaPipe Hands â†’ 21 3D Landmarks â†’ Feature Extraction (41 features) â†’ Classifier â†’ Transcript
```

**Feature extraction** computes 41 geometric features from the hand landmarks:
- Fingertip-to-wrist distances (5)
- Fingertip-to-palm distances (5)
- Finger curl angles at PIP joints (5)
- Finger extension ratios (5)
- All inter-fingertip distances (10)
- Thumb-to-finger distances (4)
- Hand orientation â€” wrist Y, vertical span, horizontal span (3)
- Finger spread angles (4)

All distances are normalized by hand size for scale invariance.

**Classification** uses weighted Euclidean distance to pre-computed sign centroids with a Gaussian kernel for scoring. Custom signs and calibrations add nearest-neighbor matching for higher accuracy.

## ğŸŒ Browser Support

| Browser | Webcam | Demo Mode |
|---------|--------|-----------|
| Chrome 90+ | âœ… | âœ… |
| Edge 90+ | âœ… | âœ… |
| Firefox | âš ï¸ Limited | âœ… |
| Safari | âš ï¸ Limited | âœ… |

Chrome or Edge recommended for best MediaPipe performance.

## ğŸ“ Project Structure

```
signbridge/
â”œâ”€â”€ index.html    â† entire app (HTML + CSS + JS, self-contained)
â”œâ”€â”€ README.md     â† this file
â””â”€â”€ LICENSE        â† MIT
```

## âš ï¸ Limitations

- **Static gestures only** â€” recognizes hand shapes, not motion-based signs (which require temporal sequence modeling)
- **Single hand primary** â€” classifier uses the first detected hand; two-hand signs are approximated
- **Synthetic training data** â€” built-in centroids are based on idealized gesture geometry, not real signer data. Calibration significantly improves accuracy for your specific setup
- **No fingerspelling** â€” individual letter recognition (A-Z) is not included in the current vocabulary

## ğŸ™ Acknowledgments

- [MediaPipe Hands](https://google.github.io/mediapipe/solutions/hands.html) â€” Google's real-time hand tracking
- ASL gesture references from the Deaf community and educational resources

## ğŸ“„ License

MIT â€” use it however you like.
